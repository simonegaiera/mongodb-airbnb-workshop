# Default values for litellm
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

enabled: true

replicaCount: 2

image:
  repository: ghcr.io/berriai/litellm
  pullPolicy: IfNotPresent
  tag: "main-latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: false
  automount: true
  annotations: {}
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

service:
  type: ClusterIP
  name: litellm-service
  port: 4000
  targetPort: 4000
  protocol: TCP

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources:
  limits:
    cpu: 500m
    memory: 1Gi
  requests:
    cpu: 100m
    memory: 512Mi

livenessProbe: {}
readinessProbe: {}

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 85

nodeSelector: {}

tolerations: []

affinity: {}

# LiteLLM specific configuration
litellm:
  # Environment variables
  env:
    PORT: "4000"
    LITELLM_LOG: "INFO"
  
  # API keys - these should be provided as secrets externally
  secrets:
    anthropicApiKey: ""
    azureOpenaiApiKey: ""
  
  # LiteLLM configuration
  config:
    model_list:
      - model_name: "claude-3-haiku"
        litellm_params:
          model: "anthropic/claude-3-haiku-20240307"
          api_key: "os.environ/ANTHROPIC_API_KEY"
          max_tokens: 4096
          temperature: 0.7
          cache_control_injection_points:
            - location: message
              role: system
      - model_name: "claude-4-sonnet"
        litellm_params:
          model: "anthropic/claude-sonnet-4-20250514"
          api_key: "os.environ/ANTHROPIC_API_KEY"
          max_tokens: 4096
          temperature: 0.7
          cache_control_injection_points:
            - location: message
              role: system
      - model_name: "gpt-5-mini"
        litellm_params:
          model: "azure/gpt-5-mini"
          api_key: "os.environ/AZURE_OPENAI_API_KEY"
          api_base: "https://solutionsconsultingopenai.openai.azure.com"
          api_version: "2025-04-01-preview"
          max_tokens: 4096
          temperature: 0.7
          cache_control_injection_points:
            - location: message
              role: system
    
    general_settings:
      disable_spend_logs: false
      disable_master_key_return: true
      max_input_tokens: 32000
      max_output_tokens: 4096
      default_max_tokens: 4096
      enforce_max_tokens: true
    
    litellm_settings:
      set_verbose: false
      json_logs: true
      cache: true  # Enable proxy-level response caching
      disable_prompt_caching: false
      # Enable caching globally
      cache_params:
        type: "redis"  # Use Redis for response caching
        ttl: 3600  # Cache responses for 1 hour (3600 seconds)
        namespace: "litellm.cline.cache"  # Namespace for Cline-specific caching
        supported_call_types: ["completion", "acompletion", "embedding", "aembedding"]
      disable_extended_thinking: true
      disable_image_generation: true
      disable_vision: true
      drop_params:
        - "extra_headers"
        - "images"
        - "image_url"
        - "image_data"
