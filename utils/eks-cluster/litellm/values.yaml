# Default values for litellm
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

enabled: true

replicaCount: 2

image:
  repository: ghcr.io/berriai/litellm
  pullPolicy: IfNotPresent
  tag: "main-stable"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: false
  automount: true
  annotations: {}
  name: ""

podAnnotations: {}
podLabels: {}

podSecurityContext: {}

securityContext: {}

service:
  type: ClusterIP
  name: litellm-service
  port: 4000
  targetPort: 4000
  protocol: TCP

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources:
  limits:
    cpu: 4000m
    memory: 6Gi
  requests:
    cpu: 2000m
    memory: 4Gi

livenessProbe: {}
readinessProbe: {}

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 90

nodeSelector: {}

tolerations: []

affinity: {}

# LiteLLM specific configuration
litellm:
  # Environment variables
  env:
    PORT: "4000"
    LITELLM_LOG: "INFO"
  
  # API keys - these should be provided as secrets externally
  secrets:
    anthropicApiKey: ""
    azureOpenaiApiKey: ""
  
  # LiteLLM configuration
  config:
    general_settings:
      disable_spend_logs: false
      disable_master_key_return: true
      max_input_tokens: 32000
      max_output_tokens: 4096
      default_max_tokens: 4096
      enforce_max_tokens: true
    
    litellm_settings:
      set_verbose: false
      json_logs: true
      cache: false 
      disable_prompt_caching: false
      # Enable caching globally
      cache_params:
        type: "in-memory"
        ttl: 1800
      disable_extended_thinking: true
      disable_image_generation: true
      disable_vision: true
      drop_params:
        - "extra_headers"
        - "images"
        - "image_url"
        - "image_data"
